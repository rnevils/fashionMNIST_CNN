{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nd-tensor:\n",
    " - n = number of indicies requierd to access a specific element\n",
    " \n",
    "dimension (d) of a tensor DIFFERS from the dimension of a vector in a vector space\n",
    " - dimension of a tensor does not tell us how many components are in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 dimensional tensor with 9 components\n",
    "dd = [\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "]\n",
    "\n",
    "dd_bad = [\n",
    "    [1,2,3],\n",
    "    [4,5,6,7], # this doesn't work bc different length! this crazy thing isn't even a matrix or tensor, or is it? def not a matix but i guess can represent it with a tensor? so matrixes can only have yeah, 2 freedoms of dimension. so what is this things dimension? how many does it have?\n",
    "    [7,8,9]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank, Axis and Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank: number of dims within the tensor\n",
    "\n",
    "- ex. rank 2 tensor = matrix = 2d array = 2d tensor\n",
    "    \n",
    "- tells us how many indicies are required to refer to specific element\n",
    "    \n",
    "Axis: a specific dim of a tensor\n",
    "\n",
    "- rank 2 tensor has 2 axis\n",
    "- elements \"run\" along an axis\n",
    "- axis has a length: how many indicies are available along the axis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example, tensor with first axis of length = 3, second axis length = 4\n",
    "\n",
    "t = [\n",
    "    [1,2,3,4],\n",
    "    [4,5,6,5],\n",
    "    [7,8,9,10]\n",
    "]\n",
    "\n",
    "# axis 1\n",
    "t[0]\n",
    "t[1]\n",
    "t[2]\n",
    "\n",
    "# axis 2\n",
    "t[0][0]\n",
    "t[0][1]\n",
    "t[0][2]\n",
    "t[0][3]\n",
    "\n",
    "t[1][0]\n",
    "t[1][0]\n",
    "#etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape\n",
    "\n",
    "- determined by the length of each axis\n",
    "\n",
    "- shape is same as torch.Size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rank** of a variable is equal to length of its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape is important because it allows us to conceptualize a tensor\n",
    "- encodes all relevant informatino about axis & rank, and therefor indicies\n",
    "- a common operation is **reshaping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping (introduction)\n",
    "\n",
    "- changes the grouping of the terms, but does not change the underlying data\n",
    "\n",
    "- products of the shape must be equal!\n",
    "    - (3 * 3 ) = (1 * 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor(dd)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.reshape(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.reshape(1,9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 CNN Tensor Shape Explained\n",
    "\n",
    "- Putting conecepts of rank, axis and shape to use with example\n",
    "- Consider image input to a CNN\n",
    " - shape of CNN input typically has length of 4 (rank 4 tensor with 4 axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Height = 28\n",
    "Width = 28\n",
    "Color = 1 # or 3 for RGB\n",
    "BatchSize = 3\n",
    "\n",
    "example_input = [BatchSize, Color, Height, Width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an image input passes through a convolutional layer, the **shape** and underlying data are changed by the convolution operation.\n",
    "\n",
    "It changes the height and width, as well as the color channel\n",
    "- ex the color channels change to \"feature channels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 PyTorch Tensors Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch specific attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor()\n",
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "- CPU and GPU have their own data types\n",
    "- ex. 32-bit floating point (torch.float32) on cpu is torch.FloatTensor but on GPU is torch.cuda.FloatTensor\n",
    "- tensor operations between tensors must operate with tensors of same data type\n",
    "- tensor operations between tensors must operate with tensors on same device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(data) # Class constructor, uses global default dtype value (torch.get_default_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data) # factory function, infers the dtype (type inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(data) # another factory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(data) # another factory function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating tensor without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2) # identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0775, 0.3810],\n",
       "        [0.8816, 0.2963]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Creating PyTorch Tensors for Deep Learning Best options\n",
    "\n",
    "factory functions\n",
    "\n",
    "- a function that accepts parameter inuputs and returns a particular type of object\n",
    "- purpose is to allow for more dynamic object creation?\n",
    "- def google this and see more into it....\n",
    "\n",
    "\n",
    "- factory functions (for pytorch) have better documentation and more configuration parameters so lets use those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in methods, sharing vs copying memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the data\n",
    "data[0] = 0\n",
    "data[1] = 0\n",
    "data[2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([0, 0, 0])\n",
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharing the data is more effecient\n",
    "\n",
    "\"best\" function is torch.tensor(), its the go-to\n",
    "\n",
    "tuning code for performance, as_tensor\n",
    "\n",
    "so as_tensor vs from_numpy?\n",
    "- as_tensor can accept any data type, not just numpy. Otherwise they are mostly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Flatten, Reshape, and Squeeze Explained\n",
    "\n",
    "### Tensor operations\n",
    "1. reshaping\n",
    "2. element-wise operations\n",
    "3. reduction operations\n",
    "4. access operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 3.],\n",
       "         [3., 3., 3.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(2,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeezing and unsqueezing\n",
    "\n",
    "squeezing: remove all of the axis that have a length of 1\n",
    "\n",
    "unsqueezing: adds a dimension with a length of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12))\n",
    "print(t.reshape(1,12).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12).squeeze())\n",
    "print(t.reshape(1,12).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0))\n",
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flattening\n",
    "- a specific type of reshaping\n",
    "- removes all the axes except for 1 (turns into a big vector, rank 1 tensor)\n",
    "- common use case is when transitioning from a convolutional layer to a fully connected layer\n",
    " - take output in form of output channels, and flatten into a 1d-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = t.reshape(1,-1) # the -1 means figure out what it should be. For ours, it will figure out it should be 12\n",
    "    t = t.squeeze()\n",
    "    return t\n",
    "\n",
    "# or \n",
    "def flatten2(t):\n",
    "    t = t.reshape() # dim= 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 CNN Flatten Operation Visualized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example images output from different channels from cnn\n",
    "\n",
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine them\n",
    "t = torch.stack((t1, t2, t3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[2, 2, 2, 2],\n",
       "         [2, 2, 2, 2],\n",
       "         [2, 2, 2, 2],\n",
       "         [2, 2, 2, 2]],\n",
       "\n",
       "        [[3, 3, 3, 3],\n",
       "         [3, 3, 3, 3],\n",
       "         [3, 3, 3, 3],\n",
       "         [3, 3, 3, 3]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1, 1],\n",
       "          [1, 1, 1, 1],\n",
       "          [1, 1, 1, 1],\n",
       "          [1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[2, 2, 2, 2],\n",
       "          [2, 2, 2, 2],\n",
       "          [2, 2, 2, 2],\n",
       "          [2, 2, 2, 2]]],\n",
       "\n",
       "\n",
       "        [[[3, 3, 3, 3],\n",
       "          [3, 3, 3, 3],\n",
       "          [3, 3, 3, 3],\n",
       "          [3, 3, 3, 3]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a color channel\n",
    "t = t.reshape(3,1,4,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then flatten\n",
    "t.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# however this doesn't work because \n",
    "# because we need individual predictions\n",
    "# for each image within our batch tensor, and now we have a flattened mess\n",
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 1 is an index, so it is the color axis, it skipps the batch axis\n",
    "t.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Tensors for Deep Learning - Broadcasting and Element-wise Operations \n",
    "\n",
    "Element-wise Operations: operations that operates on elements with same index between tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [9,8],\n",
    "    [7,6]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 tensors must have the same **shape** to do an element-wise operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adittion\n",
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor broadcasting\n",
    "- the scalar turns into an appropriate tensor to match shape, then element wise\n",
    "- important in data preprocessing and normalization\n",
    " - don't write loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + 2 # or t1.add(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trickier broadcasting\n",
    "\n",
    "t1 = torch.tensor([\n",
    "    [1,1],\n",
    "    [1,1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [3., 5.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 Code for Deep Learning - ArgMax and Reduction Tensor Ops\n",
    "\n",
    "reduction operation\n",
    "- reduces number of elements contained within tensor\n",
    "- operates on elements within a single tensor\n",
    "- sum,mean,std, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(t.sum())\n",
    "print(t.sum().numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with respect to a specific axis\n",
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argmax\n",
    "- which argument when supplied as input, gives functions max output\n",
    "- use it on the NN's output prediction tensor. allows us to determine which category has highest prediction value (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,0,0,2],\n",
    "    [0,3,3,0],\n",
    "    [4,0,0,5]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4., 3., 3., 5.]),\n",
       "indices=tensor([2, 1, 1, 2]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2., 3., 5.]),\n",
       "indices=tensor([3, 2, 3]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 3])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 Data in Deep Learning\n",
    "\n",
    "- fashion MNIST similar to same (10 classes)\n",
    "- same image size\n",
    "- same split of 60,000 train and 10,000 test\n",
    "    - NNs that have been trained on original MNIST can be transfered to this one without changing the output layers\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 CNN Image Preparation Code Project - Learn to Extract, Transform, Load (ETL)\n",
    "\n",
    "- using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 pytorch classes for this\n",
    "Dataset\n",
    "- an abstract class for representing a dataset\n",
    "\n",
    "DataLoader\n",
    "- wrapps a datsetset and provides access to underlying data\n",
    "\n",
    "An **Abstract class** is a class that has methods we **must** implement ourselves\n",
    "\n",
    "So to create a custom dataset using pytorch, we just extend the Dataset class by creating a subclass the implrments the required methods.\n",
    "\n",
    "Upon doing this, the new class can then be passed to a DataLoader object constructor, thereby wrapping the dataset and giving adittinoal functionality.\n",
    "\n",
    "We will be using fashion-MNIST so don't need to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision\n",
    "\n",
    "- gives us access to Datasets (MNIST), Models (VGGA16), Trandorms, Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc35a460cec4b49a417a129ebae4bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5544b96e52be4c0aa7b6a00169733991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afaa331f4d04a32b516a2fc91f0eca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474a659478134b2eaa684d045a9faf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# get a dataset, then wrap it with a DataLoader\n",
    "\n",
    "# get fashion-MNIST\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping the train set into the DataLoader object instance, now can use Loader for tasks,\n",
    "# like batch size, thread management and shuffeling\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set\n",
    "    ,batch_size=10\n",
    "#     ,shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 PyTorch Datasets and DataLoaders - Training Set Exploration\n",
    "\n",
    "working with the dataset and dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uniformly distributed wrt number of samples for each category\n",
    "- in general, should mirror what they should be in real world or equal number\n",
    "- replicate less common one until there are equal number (paper about how ***its always the best thing to do)***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing a data sample\n",
    "sample = next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(label).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print( 'label:' , label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17 - Build PyTorch CNN - Object Oriented Neural Networks\n",
    "\n",
    "- we are now ready to **build the model**\n",
    "\n",
    "### object oriented programming \n",
    "- every pytorch model is OOP\n",
    "- a layer is an object\n",
    "- an entire nn is also an object\n",
    "    - but layers are just functions, and nn are just composition of layers so composition of a function is just a function\n",
    "- nn.Module is the base class. Your models should also subclass this class\n",
    "- must provide an implementation if the forward method\n",
    "    - usually use nn.functional package to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 - CNN Layers \n",
    "\n",
    "- hyperparameters\n",
    "    - kernel_size: sets size of filter\n",
    "    - out_channels: number of filters\n",
    "    - out_features: size of output tensor\n",
    "    \n",
    "- data dependent hyperameters:\n",
    "    - in_channels\n",
    "    - out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19 - CNN Weights\n",
    "\n",
    "- learnable parameters: different from hyperparameter in that the value is learned during training process. Then get updated when network \"learns\" those values for the parameters. Values that minimize the loss function\n",
    "    - weights that live inside each layer\n",
    "    - bias are also learnable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "# using the print method from module\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stride and bias we did not set, so it was just set to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1224,  0.0843, -0.1443,  0.1261, -0.1108],\n",
       "          [-0.1631, -0.1556, -0.0752,  0.0667, -0.1488],\n",
       "          [ 0.0591, -0.0951, -0.0452, -0.1542,  0.1839],\n",
       "          [ 0.0633,  0.1150,  0.1933, -0.0303,  0.0538],\n",
       "          [ 0.0831, -0.1979,  0.1889, -0.1542,  0.0771]]],\n",
       "\n",
       "\n",
       "        [[[-0.0792,  0.0255, -0.1292, -0.0770, -0.1758],\n",
       "          [-0.1069,  0.0555, -0.0490,  0.1635,  0.0453],\n",
       "          [-0.1097, -0.0551, -0.0295,  0.0210,  0.0351],\n",
       "          [-0.0315,  0.0182, -0.0163,  0.0439, -0.1831],\n",
       "          [-0.0748, -0.0650, -0.0057,  0.0115, -0.0422]]],\n",
       "\n",
       "\n",
       "        [[[-0.1487,  0.1980,  0.0723, -0.0853,  0.1570],\n",
       "          [ 0.0238, -0.1202,  0.1718,  0.1184, -0.1700],\n",
       "          [-0.1698,  0.1335,  0.1330, -0.0457,  0.0128],\n",
       "          [-0.1484, -0.1591,  0.1481,  0.1568,  0.0005],\n",
       "          [ 0.0394, -0.0581,  0.1898,  0.0773, -0.0074]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0138, -0.0685,  0.1984,  0.1841,  0.0955],\n",
       "          [-0.1600, -0.1316, -0.0064, -0.0305, -0.1797],\n",
       "          [-0.0410, -0.0958,  0.1632, -0.0742, -0.1091],\n",
       "          [ 0.1485,  0.1012, -0.1449, -0.1030,  0.1796],\n",
       "          [ 0.1393,  0.0263, -0.0729, -0.1285,  0.0492]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1728,  0.1781, -0.1725,  0.1530,  0.1398],\n",
       "          [ 0.1239,  0.0526, -0.1319, -0.0133,  0.0029],\n",
       "          [ 0.0520,  0.1850, -0.0914, -0.1022,  0.1217],\n",
       "          [-0.0874,  0.1141,  0.1820,  0.0922,  0.1929],\n",
       "          [-0.0643, -0.1069,  0.1734,  0.0577, -0.1251]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1156, -0.0484, -0.0555,  0.0642, -0.1992],\n",
       "          [ 0.1253,  0.0033,  0.1283,  0.1191, -0.0940],\n",
       "          [-0.1056, -0.1698, -0.1387,  0.1972,  0.1854],\n",
       "          [ 0.1993,  0.0943,  0.0508,  0.1718,  0.1851],\n",
       "          [-0.0677,  0.0828,  0.0186,  0.1963,  0.0182]]]], requires_grad=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookint at weights\n",
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter class extends the tensor class and its when the weights are learned and automatically updated. notice it says \"Parameter containing:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note the filters are all represented as 1 tensor, the 6 filters. Then 1 \"input channel\" or depth, and then 5x5 height and width of the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal weight matrix, commonly known "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 - Callable Neural Networks - Linear Layers in Depth\n",
    "\n",
    "\n",
    "- weight matrix defines a function that maps input tensor to output tensor\n",
    "- weight matrix lives inside the linear layer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "\n",
    "# out example weight matrix\n",
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7590, 0.7040, 0.7327], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch initializes weight matrix with ransom values\n",
    "- we can explicitly set the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight = nn.Parameter(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30.3958, 40.1741, 50.1619], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get exact match if we set bias = False\n",
    "fc = nn.Linear(in_features=4, out_features=3, bias = False)\n",
    "fc.weight = nn.Parameter(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\__call\\__ method\n",
    "- notice how we could just call fc() like it was a function\n",
    "- because it implements \\__call\\__\n",
    "    - call method in turn invokes the forward() method for us\n",
    "        - which in turn invoves the linear() function which does the linear alg matrix mul\n",
    "- so any time we want to use the forward method, we just call the object instead (layers and networks, cause they are both nn.Module objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21 - CNN Forward Method implementation\n",
    " - some operations doesn't use the weights, so we call it from Functional package\n",
    " - whole network is just a composition of functions\n",
    " \n",
    " - need to reshape after conv layer and going to linear layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)  # doesn't use the weights so call from Functional package\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2) # doesn't use the weights so call from Functional package\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (4) hidden linear layer\n",
    "        # 12 is from number of output channels from previous conf layer.\n",
    "        # 4x4 is the height and width of each output channel\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        \n",
    "        # softmax returns positive probability for all possible outputs, all sum to 1\n",
    "        # usually use in final output layer\n",
    "        # we won't use it tho b/c its a loss function we will already be using in the trainig process\n",
    "        # it bacially will already be implemented in training \n",
    "        # cross-entropy loss func implicitly does softmax??\n",
    "        \n",
    "        #t = F.softmax(t, dim=1) \n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22 - CNN Image Prediction Forward Propagation Explained\n",
    "\n",
    "\n",
    "* so entire reason why it needs tensors is because we use batches!!!?? so it can do it all at once? cause like everything else just seems like it would be matrix (2d) multipliying right. Ok so also easier to represent data, like how image has color channels, plus height and width. Like you COULD just put the hwole thing in a matrix or even a vector, but less clear. thats what ends up happening anyways with flatten() operation\n",
    "\n",
    "- lets understand how our network is working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- turning off gradient calculation\n",
    " - prevents it from crating a computation graph\n",
    "\n",
    "- computation graph\n",
    "    - keeps track of network mapping and used to calculate gradients and used to update weights\n",
    "    \n",
    "- turning it off reduces memory consumption\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x12366a390>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_set)) \n",
    "image, label = sample \n",
    "image.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are just testing 1 image, but we need a batch, so we just pack an image into a batch of 1\n",
    "\n",
    "# the 4 methods in the pytorch NN module conv layers classes expect layers to have 4 dims\n",
    "\n",
    "# unsqueeze to add an adittional dim\n",
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shape needs to be (batch_size × in_channels × H × W)\n",
    "pred = network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0774, -0.0009,  0.1230,  0.0237, -0.0658,  0.0440,  0.0327,  0.0347,  0.0762, -0.0456]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 image in the batch, and 10 possible final outputs\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0911, 0.0983, 0.1113, 0.1007, 0.0921, 0.1028, 0.1017, 0.1019, 0.1062, 0.0940]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want those outputs to be probabilities, we just use softmax()\n",
    "# noticed that most are around 10% cause yeah its untrained\n",
    "F.softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23 - Neural Network Batch Processing\n",
    "\n",
    "### now, can just do the same thing with a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0774, -0.0009,  0.1230,  0.0237, -0.0658,  0.0440,  0.0327,  0.0347,  0.0762, -0.0456],\n",
       "        [-0.0766, -0.0031,  0.1179,  0.0141, -0.0685,  0.0436,  0.0415,  0.0325,  0.0856, -0.0508],\n",
       "        [-0.0647,  0.0029,  0.1155,  0.0347, -0.0575,  0.0446,  0.0597,  0.0380,  0.0857, -0.0703],\n",
       "        [-0.0657,  0.0016,  0.1193,  0.0283, -0.0629,  0.0411,  0.0568,  0.0313,  0.0919, -0.0672],\n",
       "        [-0.0729,  0.0036,  0.1222,  0.0253, -0.0745,  0.0320,  0.0448,  0.0291,  0.0982, -0.0619],\n",
       "        [-0.0748, -0.0009,  0.1309,  0.0235, -0.0633,  0.0412,  0.0332,  0.0288,  0.0816, -0.0555],\n",
       "        [-0.0664,  0.0013,  0.1303,  0.0333, -0.0577,  0.0464,  0.0439,  0.0285,  0.0755, -0.0528],\n",
       "        [-0.0813, -0.0082,  0.1290,  0.0202, -0.0637,  0.0373,  0.0222,  0.0232,  0.0810, -0.0463],\n",
       "        [-0.0653,  0.0021,  0.1236,  0.0373, -0.0553,  0.0582,  0.0590,  0.0410,  0.0914, -0.0754],\n",
       "        [-0.0699,  0.0010,  0.1305,  0.0322, -0.0599,  0.0514,  0.0427,  0.0379,  0.0835, -0.0579]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False,  True, False,  True, False, False])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24 - CNN Output Size Formula\n",
    "\n",
    "looking at how the tensor is tranformed as it moved throughout the network\n",
    "\n",
    "whoah cool debugging thing in vs code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25 - CNN Training with Code Example\n",
    "\n",
    "- can do more than 1 epoch (a complete pass of all training examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating loss for a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # Pass Batch\n",
    "loss = F.cross_entropy(preds, labels) # Calculate Loss, the cross_entropy is one of many loss functions\n",
    "\n",
    "loss.backward() # Calculate Gradients\n",
    "# after this step, now a gradient tensor has been created, with the same\n",
    "# shape as the weight tensor! (a gradient for each weight)\n",
    "\n",
    "\n",
    "optimizer.step() # Update Weights\n",
    "# step in direction of loss function minimum\n",
    "\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss2:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27 - Stack vs Concat in PyTorch\n",
    "\n",
    "examples:\n",
    "- suppose we have 3 images as tensors (channel, height, width). We want to join these together to form a batch of 3 images. Do we concat or stack?\n",
    "    - stack b/c we need new dimension for batch\n",
    "- same 3 images, but now they have a dimension for the batch\n",
    "    - now we can just concat using that existing dimension\n",
    "- same image tensors from first (no batch dim), but we want to join them with existing batch\n",
    "    - we need to first stack the 3 images, then concat with existing batch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1,1,1])\n",
    "t2 = torch.tensor([2,2,2])\n",
    "t3 = torch.tensor([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2, 2, 2, 3, 3, 3])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
